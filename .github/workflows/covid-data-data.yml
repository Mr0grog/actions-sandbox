# This is a basic workflow to help you get started with Actions

name: data.json update

# Controls when the action will run. Triggers the workflow on push or pull request
# events but only for the master branch
on:
  push:
    branches: [ master ]
  # schedule:
  #   - cron: 0 */3 * * *

jobs:
  # This workflow contains a single job called "build"
  build:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
    # Checkout the main repo
    - name: Checkout Main repo
      uses: actions/checkout@v2
      with:
        path: site

    # Checkout data scraper repo
    - name: Checkout Data Scraper
      uses: actions/checkout@v2
      with:
        repository: sfbrigade/data-covid19-sfbayarea
        path: scraper

    # Install Python
    - name: Set up Python 3.x
      uses: actions/setup-python@v1
      with:
        python-version: '3.x'
    
    # Cache Dependencies
    - name: Get pip cache dir
      id: pip-cache
      run: |
        echo "::set-output name=dir::$(pip cache dir)"
    - name: Setup Pip Caching
      uses: actions/cache@v2
      with:
        path: ${{ steps.pip-cache.outputs.dir }}
        key: ${{ runner.os }}-pip-${{ hashFiles('scraper/requirements.txt') }}-${{ hashFiles('scraper/requirements-dev.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ hashFiles('scraper/requirements.txt') }}-
          ${{ runner.os }}-pip-
    
    # Install dependencies
    # - The commit that was checked out will be available as $SCRAPER_COMMIT.
    - name: Install Data Scraper & Dependencies
      run: |
        cd ${GITHUB_WORKSPACE}/scraper
        python -m pip install --upgrade pip
        pip install -r requirements.txt;
        # Keep track of the version used so we can use it in commit messages
        echo "::set-env name=SCRAPER_COMMIT::$(git rev-parse HEAD)"

    # - name: Scrape Data
    #   # Use a custom shell so individual commands don't fail. We want to run
    #   # each county independently and keep the output from the ones that
    #   # succeed. We'll keep track of what failed and manually fail at the end.
    #   # at the end.
    #   shell: bash {0}
    #   run: |
    #     echo "::set-env name=SCRAPER_TIME::$(date)"

    #     counties='alameda contra_costa marin napa san_francisco san_mateo santa_clara solano sonoma'
    #     cd ${GITHUB_WORKSPACE}/scraper
    #     mkdir county_output
    #     OUTPUT_FILE="${GITHUB_WORKSPACE}/site/data/data.v2.json"
    #     ERROR=''
    #     for county in $counties; do
    #       python scraper_data.py "${county}" > "county_output/${county}.json"
    #       if [ "${?}" -ne 0 ]; then ERROR=true; fi
    #     done

    #     # Merge all the new data on top of the old data and save.
    #     cp "${OUTPUT_FILE}" base.json
    #     jq -s 'reduce .[] as $county ({}; . + $county)' base.json county_output/*.json > "${OUTPUT_FILE}"

    #     # Exit with an error if any of the scrapers failed.
    #     if [ -n "${ERROR}" ]; then
    #       exit 1
    #     fi

    - name: Scrape Data
      run: |
        echo "::set-env name=SCRAPER_TIME::$(date)"
        cd ${GITHUB_WORKSPACE}/scraper
        OUTPUT_FILE="${GITHUB_WORKSPACE}/site/data/data.v2.json"
        # python scraper_data.py > "${GITHUB_WORKSPACE}/site/data/data.v2.json"

    - name: Create PR if New Data
      uses: peter-evans/create-pull-request@v2
      with:
        path: site
        # If this branch already exists, it will be replaced with a commit
        # based on the changes from this job. If there's already a PR for it,
        # the PR will be updated. The end result is that each run of this job
        # supersedes or replaces any previous runs that haven't been reviewed
        # and merged yet.
        branch: auto-update-data
        title: 'GitHub action: data update'
        body: |
          Created using commit [${{env.SCRAPER_COMMIT}}](https://github.com/sfbrigade/data-covid19-sfbayarea/commit/${{env.SCRAPER_COMMIT}})
          from sfbrigade/data-covid19-sfbayarea.
          
          Scraped at: ${{ env.SCRAPER_TIME }}
        commit-message: |
          GitHubAction: data update
          
          Created with commit ${{env.SCRAPER_COMMIT}} from sfbrigade/data-covid19-sfbayarea
          https://github.com/sfbrigade/data-covid19-sfbayarea/commit/${{env.SCRAPER_COMMIT}}
        reviewers: Mr0grog
